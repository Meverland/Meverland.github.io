## 正则化

- 正则化就是对最小化经验误差函数上加约束，这样的约束可以解释为先验知识（正则化参数等价于对参数引入先验分布）

- 主要解决的问题
  - 正则化就是对最小化经验误差函数上加约束，这样的约束可以解释为先验知识（正则化参数等价于对参数引入先验分布）。约束有引导作用，在优化误差函数的时候倾向于选择满足约束的梯度减少的方向，使最终的解倾向于符合先验知识（如一般的l-norm先验，表示原问题更可能是比较简单的，这样的优化倾向于产生参数值量级小的解，一般对应于稀疏参数的平滑解）。
  - 同时，正则化解决了逆问题的不适定性，产生的解是存在，唯一同时也依赖于数据的，噪声对不适定的影响就弱，解就不会过拟合，而且如果先验（正则化合适，则解就倾向于是符合真解（更不会过拟合了），即使训练集中彼此间不相关的样本数很少。



- 正则化项可以**取不同形式**。在回归问题中，损失数是平方损失，正则化可以是参数向量L2范数：

- L2**范数**：（|x1|2次方+|x2|2次方+...+|xn|n次方）的1/2次方



![](https://s2.loli.net/2022/05/21/efLGO7qCsBSbcjW.png)

||w||表示参数向量的L2范数

也可以取L1范数

- 在机器学习中 正则化是应对过拟合的有效方法。使原本该等于0的参数尽量往0靠近。为了达到这个目的，在原来**损失函数**里加入**惩罚项 **a>0为惩罚权重。惩罚项会随着a、b、c绝对值的增大而增大。通俗来说，就是模型参数估计值越远离0，惩罚就越大。因此在估计参数时，也就是在寻找L的最小值时，这一项会迫使参数向0靠近，而且跟{y}越不相关的变量，其对应的参数值向0靠近的步伐也就越大

- 惩罚项||w||， 其前面的r/2 就是设定的权重

![img](https://s2.loli.net/2022/05/21/Qbh1zGgKq84RoIf.png)